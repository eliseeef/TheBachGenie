{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a550663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78343dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "SEASONS = \"Seasons\"\n",
    "# The Bachelor Wikipedia \n",
    "WIKIPEDIA = \"https://en.wikipedia.org/\"\n",
    "BACHELOR_WIKI = \"wiki/The_Bachelor_(American_TV_series)\"\n",
    "\n",
    "CLASS_TAG = \"class\"\n",
    "SEASONS_TABLE_CLASS = \"wikitable plainrowheaders\"\n",
    "CONTESTANTS_TABLE_CLASS = \"wikitable sortable\"\n",
    "TABLE_ROW_TAG = \"tr\"\n",
    "TABLE_DATA_TAG = \"td\"\n",
    "TABLE_HEADER_TAG = \"th\"\n",
    "TABLE_TAG = \"table\"\n",
    "TABLE_BODY_TAG = \"tbody\"\n",
    "HYPERLINK_TAG = \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624e31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tag(value):\n",
    "    return str(value.text).strip()\n",
    "\n",
    "def get_row_values(table_value):\n",
    "    column_name_list = []\n",
    "    for value in table_value:\n",
    "        column_name_list.append(str(value.text).strip())\n",
    "    return column_name_list\n",
    "    \n",
    "def get_table_headers(rows):\n",
    "    column_names = []\n",
    "    for row in rows:\n",
    "        table_headers = row.find_all(TABLE_HEADER_TAG)\n",
    "        if table_headers:\n",
    "            column_names = get_row_values(table_headers)   \n",
    "            return column_names\n",
    "    return column_names\n",
    "\n",
    "def get_hyper_links(row, season_refs):\n",
    "    hrefs = row.find_all(HYPERLINK_TAG)\n",
    "    if len(hrefs) > 2:\n",
    "        # I ONLY WANT THE FIRST REF BECAUSE IT IS RELATED TO THE SEASON\n",
    "        first_tag = str(hrefs[0])\n",
    "        soup = BeautifulSoup(first_tag, 'html.parser')\n",
    "        href_value = soup.a['href']\n",
    "        season_refs.append(href_value)\n",
    "\n",
    "def get_table_data(rows, season_refs):\n",
    "    data_list = []\n",
    "    for row in rows:\n",
    "        get_hyper_links(row, season_refs)\n",
    "        table_data = row.find_all(TABLE_DATA_TAG)\n",
    "        if table_data:\n",
    "            new_row_values = get_row_values(table_data)\n",
    "            # NOTE: RUNNERS UP ISNT COMPLETELY ACCURATE\n",
    "            # BUT IT DOESN'T MATTER BECAUSE WE ONLY CARE ABOUT WINNERS\n",
    "            if len(new_row_values) > 1:\n",
    "                data_list.append(new_row_values)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "387cf512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE I AM CREATING THE SEASONS CSV TO LOOK LIKE THE TABLE HERE\n",
    "# https://en.wikipedia.org/wiki/The_Bachelor_(American_TV_series)#Seasons\n",
    "\n",
    "html = requests.get(WIKIPEDIA+BACHELOR_WIKI)\n",
    "soup = BeautifulSoup(html.text)\n",
    "season_refs = []\n",
    "\n",
    "seasons_table = soup(TABLE_TAG, {CLASS_TAG:SEASONS_TABLE_CLASS})\n",
    "# create some new soup so we can use the find_all method\n",
    "new_soup = BeautifulSoup(str(seasons_table), 'html.parser')\n",
    "\n",
    "table_rows = new_soup.find_all(TABLE_ROW_TAG)\n",
    "\n",
    "column_names = get_table_headers(table_rows)\n",
    "data_list = get_table_data(table_rows, season_refs)\n",
    "\n",
    "# create data frame from wikipedia article\n",
    "df = pd.DataFrame(data_list, columns=column_names)\n",
    "df.to_csv('./data/seasons.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d08fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF  /wiki/The_Bachelor_(American_season_1)\n",
      "REF  /wiki/The_Bachelor_(American_season_2)\n",
      "REF  /wiki/The_Bachelor_(American_season_5)\n",
      "REF  #cite_note-12\n",
      "REF  /wiki/Charlie_O%27Connell\n",
      "REF  /wiki/The_Bachelor_(American_season_9)\n",
      "REF  /wiki/The_Bachelor_(American_season_10)\n",
      "REF  /wiki/The_Bachelor_(American_season_11)\n",
      "REF  /wiki/The_Bachelor_(American_season_12)\n"
     ]
    }
   ],
   "source": [
    "# HERE I AM CREATING THE CONTESTANTS CSV TO LOOK LIKE THE TABLE HERE\n",
    "# https://en.wikipedia.org/wiki/The_Bachelor_(American_season_1)\n",
    "# for each available season\n",
    "for ref in season_refs:\n",
    "    html = requests.get(WIKIPEDIA+ref)\n",
    "    soup = BeautifulSoup(html.text)\n",
    "\n",
    "    contestants_table = soup(TABLE_TAG, {CLASS_TAG:CONTESTANTS_TABLE_CLASS})\n",
    "\n",
    "    # create some new soup so we can use the find_all method\n",
    "    new_soup = BeautifulSoup(str(contestants_table), 'html.parser')\n",
    "\n",
    "    table_rows = new_soup.find_all(TABLE_ROW_TAG)\n",
    "\n",
    "    column_names = get_table_headers(table_rows)\n",
    "    data_list = get_table_data(table_rows, [])\n",
    "\n",
    "    # create data frame from wikipedia article\n",
    "    contestant_df = pd.DataFrame(data_list, columns=column_names)\n",
    "    contestant_df.to_csv('./data/'+ref[6:]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b160b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
